---
title: "Run ORF, CF and XBCF"
author: "Shunkei Kakimoto"
date: ""
output: html_document
---


```{r setup, include=FALSE}
library(knitr)
# library(here)
library(modelsummary)
library(data.table)
library(tidyverse)
library(maps)
library(flextable)
library(grf)
library(RColorBrewer)
library(patchwork)
library(plotly)
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(viridis)
library(grid)
library(gridExtra)
library(flextable)
library(reticulate)
# knitr::opts_chunk$set(echo = TRUE)

# opts_knit$set(root.dir = here())
opts_knit$set(root.dir = "~/Dropbox/ResearchProject/HeterogeneousAllocation")

opts_chunk$set(
  fig.align = "center", 
  fig.retina = 5,
  warning = FALSE, 
  message = FALSE,
  cache = FALSE, # <-- T
  cache.lazy = FALSE,
  echo = TRUE
  )
```

# Objective

+ apply *Double Machine Learning(DML) ORF*, *Doubly Robust(DR) ORF*, *CF (grf)*, and *Accelerated Bayesian Causal Forests (XBCF)* to the data using `reticulate` package
+ This time, I estimated the heterogeneous treatment effects of *precipitation*
  * Because we found that *precipitation* mainly contributes to treatment effects heterogeneity 

+ The codes in the below conduct those estimations, and the estimated treatment effects from each of the methods are visualized in the bottom of this report.

## Brief description of ORF (DMLOrthoForest and DROrthoForest) of EconML

+ `DMLOrthoForest`, `DROrthoForest` : Parameter setting
  * I used the following tuning parameters for ORF modeling 
    - `n_trees = 1000`
    - `min_leaf_size = 50`
    - `max_depth = 20`
    - `subsample_ratio = 0.5`
  * **NOTE:** ORF of EconML allows for various ML methods to be used for two predictive stages, but I used the default setting
    - I'm still studying about this part 
+ `fit` : Build an orthogonal random forest from a training set (Y, T, X, W).
+ `effect` : *Calculate the heterogeneous treatment effect*  $\tau(X, T0, T1)$
  * $\tau$ â€“ Heterogeneous treatment effects on each outcome for each sample Note that when Y is a vector rather than a 2-dimensional array, the corresponding singleton dimension will be collapsed (so this method will return a vector)
+ `effect_interval`: Confidence intervals for the quantities $\tau(X, T0, T1)$ produced by the model. Available only when inference is not None, when calling the fit method.

## Brief description of XBCF

+ I refereed this website [here](https://johaupt.github.io/bart/bayesian/causal%20inference/xbcf.html)
+ I used recommended parameters for XBCF modeling


# Python setting

```{r}
#--- reticulate ---#
use_python("./myenv/bin/python")
use_virtualenv("./myenv/bin/activate_this.py")

# repl_python()
```

```{python}
## Ignore warnings
import warnings
warnings.filterwarnings("ignore")

from econml.orf import DMLOrthoForest, DROrthoForest
from econml.dml import CausalForestDML
from econml.sklearn_extensions.linear_model import WeightedLassoCVWrapper, WeightedLasso, WeightedLassoCV

# Helper imports
import numpy as np
from itertools import product
from sklearn.linear_model import Lasso, LassoCV, LogisticRegression, LogisticRegressionCV
```


# Data Preparation

```{r}
#--- import the data ---# (yearly data)
ir_data <- readRDS('./Shared/Data/ir_reg.rds') %>%
  .[source %in% c('Meter','METER','metered'),] %>%
  .[,trs:=paste(twnid,rngid,section,sep='_')] %>%
  .[,tr:=paste(twnid,rngid,sep='_')] %>%
  .[,phase1:=ifelse(year<2008,1,0)] %>%
  .[,phase2:=ifelse(year>=2008,1,0)]

#--- data ---#
data_w_W1 <- ir_data %>%
  .[Low_Tri_5mi==1, ] %>%
  .[,`:=`(
    #--- LR east vs TB (2007-2008)---#
    treat1e=ifelse(phase1==1 & nrdname=='Lower Republican' & in_east==1, 1, 0),
    #--- LR west vs TB (2007 - 2008) ---#
    treat1w=ifelse(phase1==1 & nrdname=='Lower Republican' & in_west==1, 1, 0),
    #--- LR vs TB (2008 - 2015)---#
    treat2=ifelse(((nrdname=='Lower Republican' & phase2==1) | (t5r22==1& year>=2009)), 1, 0)
    )] %>%
  .[,`:=`(
    mean_precip=mean(precip_in),
    mean_gdd=mean(gdd_in),
    mean_tmin=mean(tmin_in),
    mean_tmax=mean(tmax_in)
  ),by=wellid]

#--- variables to be used in the analysis ---#
cov_ls <- c('precip_in', 'tmin_in', 'tmax_in', 'gdd_in',
  'silt_pct', 'clay_pct', 'slope', 'kv', 'awc')

all_vars <- c(cov_ls,'usage','treat2', 'tr', 'year')

#--- set target variable ---#
target_var <- "precip_in"

#--- W1 case 3: 11 inches (LR) vs no limit (TB) (2008 - 2015) ---#
data_reg_case3 <- data_w_W1 %>%
  .[year >= 2008, ] %>%
  .[,..all_vars] %>% 
  .[usage<=40,] %>% # Why?? -> make upper limit
  .[,tr_year:=factor(paste0(tr,year))] %>% 
  na.omit()


#### ==== Training data ==== ####
Y <- data_reg_case3[,usage] %>% 
  as.array()

T <- data_reg_case3[,treat2] %>% 
  as.array()

X_train <- data_reg_case3[,cov_ls,with=FALSE] %>%
  as.matrix() %>%
  unname()


## === Testing data === ##
min_temp <- data_reg_case3[[target_var]] %>% quantile(prob=0.025)
max_temp <- data_reg_case3[[target_var]] %>% quantile(prob=0.90)

X_eval_base <- copy(data_reg_case3)[,cov_ls,with=FALSE] %>%
  as_tibble(.) %>% 
  summarize_all(mean) %>%
  data.table()

X_eval <- copy(X_eval_base) %>%
  setnames(target_var,'temp_var') %>%
  .[rep(1,1000),] %>%
  .[,temp_var:=seq(min_temp,max_temp,length=1000)] %>%
  setnames('temp_var',target_var) %>%
  # reoder columns
  .[,..cov_ls] %>%
  as.matrix() %>%
  unname()

```

# DROrthoForest (Doubly Robust ORF)

```{python}
# Define some parameters
n_trees = 1000
min_leaf_size = 50
max_depth = 20
subsample_ratio = 0.5

dr_orf_est = DROrthoForest(
    n_trees=n_trees, 
    min_leaf_size=min_leaf_size,
    max_depth=max_depth, 
    subsample_ratio=subsample_ratio
)

#--- Build an ORF from a training set ---#
dr_orf_est.fit(r.Y, r.T, X=r.X_train)

#--- Calculate the heterogeneous treatment effect ---#
dr_orf_te_pred = dr_orf_est.effect(X = r.X_eval)

#--- Calculate default (95%) confidence intervals for the default treatment points T0=0 and T1=1 ---#
dr_orf_te_lower, dr_orf_te_upper = dr_orf_est.effect_interval(r.X_eval)
```

```{r}
dr_orf_cate_effect <- tibble("value" = X_eval[,1], "tau_hat" = py$dr_orf_te_pred) %>%
   mutate(
    te_pred_down = py$dr_orf_te_lower,
    te_pred_up   = py$dr_orf_te_upper
  ) %>%
  data.table() %>%
  .[, method:="DR_ORF"]

# dr_orf_vis <- dr_orf_cate_effect %>% 
#   ggplot(aes(value, tau_hat)) +
#   geom_ribbon(aes(ymin = te_pred_down, ymax = te_pred_up),
#               fill = "grey70") +
#   geom_line() +
#   labs(
#     x = target_var,
#     y = "Treatment Effects",
#     title = "Doubly Robust ORF"
#   )
```


# DMLOrthoForest (Double Machine Learning ORF)

```{python}
dml_orf_est = DMLOrthoForest(
        n_trees=n_trees, 
        min_leaf_size=min_leaf_size, 
        max_depth=max_depth, 
        subsample_ratio=subsample_ratio
        # model_T=Lasso(alpha=0.1),
        # model_Y=Lasso(alpha=0.1),
        # model_T_final=WeightedLassoCVWrapper(cv=3), 
        # model_Y_final=WeightedLassoCVWrapper(cv=3)
       )


#--- Build an ORF from a training set ---#
dml_orf_est.fit(r.Y, r.T, X=r.X_train)

#--- Calculate the heterogeneous treatment effect ---#
dml_orf_te_pred = dml_orf_est.effect(X = r.X_eval)

#--- Calculate default (95%) confidence intervals for the default treatment points T0=0 and T1=1 ---#
dml_orf_te_lower, dml_orf_te_upper = dml_orf_est.effect_interval(r.X_eval)
```


```{r}
dml_orf_cate_effect <- tibble("value" = X_eval[,1], "tau_hat" = py$dml_orf_te_pred) %>%
   mutate(
    te_pred_down = py$dml_orf_te_lower,
    te_pred_up   = py$dml_orf_te_upper
  ) %>%
  data.table() %>%
  .[, method:="DML_ORF"]

# dml_orf_vis <- dml_orf_cate_effect %>% 
#   ggplot(aes(value, tau_hat)) +
#   geom_ribbon(aes(ymin = te_pred_down, ymax = te_pred_up),
#               fill = "grey70") +
#   geom_line() +
#   labs(
#     x = target_var,
#     y = "Treatment Effects",
#     title = "Double Machine Learning ORF"
#   )
```

# Causal Forest of GRF

```{r}
source('./GitControlled/Codes/Functions/functions.R')

#-- cluster: unique combinations of twnid,rngid, year --#
cl <- data_reg_case3[,tr_year]

Y_cf <- data_reg_case3[,usage]
T_cf <- data_reg_case3[,treat2]
X_cf <- data_reg_case3[,cov_ls,with=FALSE]


Y_forest <- regression_forest(X_cf, Y_cf, clusters=cl)
Y_hat <- predict(Y_forest)$predictions
W_forest <- regression_forest(X_cf, T_cf, clusters=cl)
W_hat <- predict(W_forest)$predictions

#=== run CF ===#
cf1 <- causal_forest(
  X_cf, Y_cf, T_cf,
  Y.hat = Y_hat, 
  W.hat = W_hat,
  clusters = cl,
  num.trees= 2000,
  tune.parameters = 'all' 
  )

#--- assessing treatment heterogeneity ---#
cf_cate_effect <- get_impact(
  cf_res = cf1,
  data_base = data_reg_case3,
  var_ls = cov_ls,
  var_name = target_var
  ) %>%
  .[,`:=`(
    te_pred_down = tau_hat-1.96*tau_hat_se,
    te_pred_up = tau_hat+1.96*tau_hat_se
    )] %>%
  .[,.(value, tau_hat, te_pred_down, te_pred_up)] %>%
  .[,method := "CF"]
  
# cf_vis <- ggplot() + 
#     geom_ribbon(data= cf_cate_effect, aes(
#       ymin = te_pred_down, 
#       ymax = te_pred_up,
#       x = value), fill = "grey70") +
#   geom_line(data= cf_cate_effect, aes(value, tau_hat)) +
#   labs(
#     x = target_var,
#     y = "Treatment Effects",
#     title = "CF of grf"
#   )
```


# Accelerated Bayesian Causal Forests (XBCF)

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from xbcausalforest import XBCF

NUM_TREES_PR  = 200
NUM_TREES_TRT = 100

xbcf = XBCF(
    #model="Normal",
    parallel=True, 
    num_sweeps=50, 
    burnin=15,
    max_depth=250,
    num_trees_pr=NUM_TREES_PR,
    num_trees_trt=NUM_TREES_TRT,
    num_cutpoints=100,
    Nmin=1,
    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'
    #mtry_trt=X.shape[1], 
    tau_pr = 0.6 * np.var(r.Y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,
    tau_trt = 0.1 * np.var(r.Y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,
    alpha_pr= 0.95, # shrinkage (splitting probability)
    beta_pr= 2, # shrinkage (tree depth)
    alpha_trt= 0.95, # shrinkage for treatment part
    beta_trt= 2,
    p_categorical_pr = 0,
    p_categorical_trt = 0,
    standardize_target=True, # standardize y and unstandardize for prediction
         )

T= r.T.astype('int32')

#--- Build an XBCF from a training set ---#
xbcf.fit(
    x_t=r.X_train, # Covariates treatment effect
    x=r.X_train, # Covariates outcome (including propensity score)
    y=r.Y,  # Outcome
    z=T, # Treatment group
)

#--- Calculate the heterogeneous treatment effect ---#
xbcf_te_pred = xbcf.predict(r.X_eval, return_mean=True) 

#--- Calculate 95% credible intervals ---#
tau_posterior = xbcf.predict(r.X_eval, return_mean=False)[:,xbcf.getParams()['burnin']:]

interval_dt = pd.DataFrame(np.quantile(tau_posterior, [0.05,0.95], axis=1).T, columns=['CI_lower', 'CI_upper'])
```


```{r}
xbcf_cate_effect <- tibble("value" = X_eval[,1], "tau_hat" = py$xbcf_te_pred) %>%
   mutate(
    te_pred_down = py$interval_dt[,"CI_lower"],
    te_pred_up   = py$interval_dt[,"CI_upper"]
  ) %>%
  data.table() %>%
  .[, method:="XBCF"]

# xbcf_vis <- dml_orf_cate_effect %>% 
#   ggplot(aes(value, tau_hat)) +
#   geom_ribbon(aes(ymin = te_pred_down, ymax = te_pred_up),
#               fill = "grey70") +
#   geom_line() +
#   labs(
#     x = target_var,
#     y = "Treatment Effects",
#     title = "XBCF"
#   )
```


# Visualization

```{r, fig.width=12, fig.height=6}
report <- bind_rows(dr_orf_cate_effect, dml_orf_cate_effect, cf_cate_effect, xbcf_cate_effect)

ggplot(report)+
  geom_ribbon(aes(
      ymin = te_pred_down, 
      ymax = te_pred_up,
      x = value), fill = "grey70") +
  geom_line(aes(value, tau_hat)) +
  facet_grid(~method) +
  labs(
    x = target_var,
    y = "Treatment Effects"
  ) 
```

## Finding
+ Treatment effects of precipitation estimated by ORF is almost consistent with CF of grf. The estimates from ORF have more tight 95% CI. 
+ The shape of the graph of XBCF is quite angular
  * need to increase the number of trees?
    - Need to read the paper more
















