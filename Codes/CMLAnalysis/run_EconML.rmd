---
title: "Run ORF, CF and XBCF"
author: "Shunkei Kakimoto"
date: ""
output: html_document
---


```{r setup, include=FALSE}
library(knitr)
# library(here)
library(modelsummary)
library(data.table)
library(tidyverse)
library(maps)
library(flextable)
library(grf)
library(RColorBrewer)
library(patchwork)
library(plotly)
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(viridis)
library(grid)
library(gridExtra)
library(here)
library(flextable)
library(reticulate)
# knitr::opts_chunk$set(echo = TRUE)


opts_knit$set(root.dir = here::i_am("GitControlled/Codes/CMLAnalysis/run_EconML.rmd"))
# opts_knit$set(root.dir = "~/Dropbox/ResearchProject/HeterogeneousAllocation")

opts_chunk$set(
  fig.align = "center",
  fig.retina = 5,
  warning = FALSE,
  message = FALSE,
  cache = FALSE, # <-- T
  cache.lazy = FALSE,
  echo = TRUE
)
```

# Objective

+ apply *Double Machine Learning(DML) ORF*, *Doubly Robust(DR) ORF*, *CF (grf)*, and *Accelerated Bayesian Causal Forests (XBCF)* to the data using `reticulate` package
+ This time, I estimated the heterogeneous treatment effects of *precipitation*
  * Because we found that *precipitation* mainly contributes to treatment effects heterogeneity 

+ The codes in the below conduct those estimations, and the estimated treatment effects from each of the methods are visualized in the bottom of this report.

## Brief description of ORF (DMLOrthoForest and DROrthoForest) of EconML

+ `DMLOrthoForest`, `DROrthoForest` : Parameter setting
  * I used the following tuning parameters for ORF modeling 
    - `n_trees = 1000`
    - `min_leaf_size = 50`
    - `max_depth = 20`
    - `subsample_ratio = 0.5`
  * **NOTE:** ORF of EconML allows for various ML methods to be used for two predictive stages, but I used the default setting
    - I'm still studying about this part 
+ `fit` : Build an orthogonal random forest from a training set (Y, T, X, W).
+ `effect` : *Calculate the heterogeneous treatment effect*  $\tau(X, T0, T1)$
  * $\tau$ â€“ Heterogeneous treatment effects on each outcome for each sample Note that when Y is a vector rather than a 2-dimensional array, the corresponding singleton dimension will be collapsed (so this method will return a vector)
+ `effect_interval`: Confidence intervals for the quantities $\tau(X, T0, T1)$ produced by the model. Available only when inference is not None, when calling the fit method.

## Brief description of XBCF

+ I refereed this website [here](https://johaupt.github.io/bart/bayesian/causal%20inference/xbcf.html)
+ I used recommended parameters for XBCF modeling


# Python setting

```{r}
#--- reticulate ---#
# use_python("./myenv/bin/python")
# use_virtualenv("./myenv/bin/activate_this.py")
# use_python("/anaconda3/bin/python")
# repl_python()
py_config()
```

```{python}
## Ignore warnings
import warnings
warnings.filterwarnings("ignore")

from econml.orf import DMLOrthoForest, DROrthoForest
from econml.dml import CausalForestDML
from econml.sklearn_extensions.linear_model import WeightedLassoCVWrapper, WeightedLasso, WeightedLassoCV

# Helper imports
import numpy as np
from itertools import product
from sklearn.linear_model import Lasso, LassoCV, LogisticRegression, LogisticRegressionCV
```


# Data Preparation

```{r }
#--- import the data ---# (yearly data)
ir_data <- 
  readRDS(here("Shared/Data/ir_reg.rds")) %>%
  .[source %in% c("Meter", "METER", "metered"), ] %>%
  .[, trs := paste(twnid, rngid, section, sep = "_")] %>%
  .[, tr := paste(twnid, rngid, sep = "_")] %>%
  .[, phase1 := ifelse(year < 2008, 1, 0)] %>%
  .[, phase2 := ifelse(year >= 2008, 1, 0)]

#--- data ---#
data_w_W1 <- ir_data %>%
  .[Low_Tri_5mi == 1, ] %>%
  .[, `:=`(
    #--- LR east vs TB (2007-2008)---#
    treat1e = ifelse(phase1 == 1 & nrdname == "Lower Republican" & in_east == 1, 1, 0),
    #--- LR west vs TB (2007 - 2008) ---#
    treat1w = ifelse(phase1 == 1 & nrdname == "Lower Republican" & in_west == 1, 1, 0),
    #--- LR vs TB (2008 - 2015)---#
    treat2 = ifelse(((nrdname == "Lower Republican" & phase2 == 1) | (t5r22 == 1 & year >= 2009)), 1, 0)
  )] %>%
  .[, `:=`(
    mean_precip = mean(precip_in),
    mean_gdd = mean(gdd_in),
    mean_tmin = mean(tmin_in),
    mean_tmax = mean(tmax_in)
  ), by = wellid]

#--- variables to be used in the analysis ---#
cov_ls <- c(
  "precip_in", "tmin_in", "tmax_in", "gdd_in",
  "silt_pct", "clay_pct", "slope", "kv", "awc"
)

all_vars <- c(cov_ls, "usage", "treat2", "tr", "year")

#--- set target variable ---#
target_var <- "precip_in"

#--- W1 case 3: 11 inches (LR) vs no limit (TB) (2008 - 2015) ---#
data_reg_case3 <- data_w_W1 %>%
  .[year >= 2008, ] %>%
  .[, ..all_vars] %>%
  .[usage <= 40, ] %>% # Why?? -> make upper limit
  .[, tr_year := factor(paste0(tr, year))] %>%
  na.omit()


#### ==== Training data ==== ####
Y <- data_reg_case3[, usage] %>%
  as.array()

T <- data_reg_case3[, treat2] %>%
  as.array()

X_train <- data_reg_case3[, cov_ls, with = FALSE] %>%
  as.matrix() %>%
  unname()


## === Testing data === ##
min_temp <- data_reg_case3[[target_var]] %>% quantile(prob = 0.025)
max_temp <- data_reg_case3[[target_var]] %>% quantile(prob = 0.90)

X_eval_base <- copy(data_reg_case3)[, cov_ls, with = FALSE] %>%
  as_tibble(.) %>%
  summarize_all(mean) %>%
  data.table()

X_eval <- copy(X_eval_base) %>%
  setnames(target_var, "temp_var") %>%
  .[rep(1, 1000), ] %>%
  .[, temp_var := seq(min_temp, max_temp, length = 1000)] %>%
  setnames("temp_var", target_var) %>%
  # reoder columns
  .[, ..cov_ls] %>%
  as.matrix() %>%
  unname()
```

# DROrthoForest (Doubly Robust ORF)

```{python}
# Define some parameters
n_trees = 1000
min_leaf_size = 50
max_depth = 20
subsample_ratio = 0.5

dr_orf_est = DROrthoForest(
    n_trees=n_trees, 
    min_leaf_size=min_leaf_size,
    max_depth=max_depth, 
    subsample_ratio=subsample_ratio
)

#--- Build an ORF from a training set ---#
dr_orf_est.fit(r.Y, r.T, X=r.X_train)

#--- Calculate the heterogeneous treatment effect ---#
dr_orf_te_pred = dr_orf_est.effect(X = r.X_eval)

#--- Calculate default (95%) confidence intervals for the default treatment points T0=0 and T1=1 ---#
dr_orf_te_lower, dr_orf_te_upper = dr_orf_est.effect_interval(r.X_eval)
```

```{r}
dr_orf_cate_effect <- tibble("value" = X_eval[, 1], "tau_hat" = py$dr_orf_te_pred) %>%
  mutate(
    te_pred_down = py$dr_orf_te_lower,
    te_pred_up   = py$dr_orf_te_upper
  ) %>%
  data.table() %>%
  .[, method := "DR_ORF"]

# dr_orf_vis <- dr_orf_cate_effect %>%
#   ggplot(aes(value, tau_hat)) +
#   geom_ribbon(aes(ymin = te_pred_down, ymax = te_pred_up),
#               fill = "grey70") +
#   geom_line() +
#   labs(
#     x = target_var,
#     y = "Treatment Effects",
#     title = "Doubly Robust ORF"
#   )
```


# DMLOrthoForest (Double Machine Learning ORF)

```{python}
dml_orf_est = DMLOrthoForest(
        n_trees=n_trees, 
        min_leaf_size=min_leaf_size, 
        max_depth=max_depth, 
        subsample_ratio=subsample_ratio
        # model_T=Lasso(alpha=0.1),
        # model_Y=Lasso(alpha=0.1),
        # model_T_final=WeightedLassoCVWrapper(cv=3), 
        # model_Y_final=WeightedLassoCVWrapper(cv=3)
       )


#--- Build an ORF from a training set ---#
dml_orf_est.fit(r.Y, r.T, X=r.X_train)

#--- Calculate the heterogeneous treatment effect ---#
dml_orf_te_pred = dml_orf_est.effect(X = r.X_eval)

#--- Calculate default (95%) confidence intervals for the default treatment points T0=0 and T1=1 ---#
dml_orf_te_lower, dml_orf_te_upper = dml_orf_est.effect_interval(r.X_eval)
```


```{r}
dml_orf_cate_effect <- tibble("value" = X_eval[, 1], "tau_hat" = py$dml_orf_te_pred) %>%
  mutate(
    te_pred_down = py$dml_orf_te_lower,
    te_pred_up   = py$dml_orf_te_upper
  ) %>%
  data.table() %>%
  .[, method := "DML_ORF"]

# dml_orf_vis <- dml_orf_cate_effect %>%
#   ggplot(aes(value, tau_hat)) +
#   geom_ribbon(aes(ymin = te_pred_down, ymax = te_pred_up),
#               fill = "grey70") +
#   geom_line() +
#   labs(
#     x = target_var,
#     y = "Treatment Effects",
#     title = "Double Machine Learning ORF"
#   )
```

# Causal Forest of GRF

```{r}
source("./GitControlled/Codes/Functions/functions.R")

#-- cluster: unique combinations of twnid,rngid, year --#
cl <- data_reg_case3[, tr_year]

Y_cf <- data_reg_case3[, usage]
T_cf <- data_reg_case3[, treat2]
X_cf <- data_reg_case3[, cov_ls, with = FALSE]


Y_forest <- regression_forest(X_cf, Y_cf, clusters = cl)
Y_hat <- predict(Y_forest)$predictions
W_forest <- regression_forest(X_cf, T_cf, clusters = cl)
W_hat <- predict(W_forest)$predictions

# === run CF ===#
cf1 <- causal_forest(
  X_cf, Y_cf, T_cf,
  Y.hat = Y_hat,
  W.hat = W_hat,
  clusters = cl,
  num.trees = 2000,
  tune.parameters = "all"
)

#--- assessing treatment heterogeneity ---#
cf_cate_effect <- get_impact(
  cf_res = cf1,
  data_base = data_reg_case3,
  var_ls = cov_ls,
  var_name = target_var
) %>%
  .[, `:=`(
    te_pred_down = tau_hat - 1.96 * tau_hat_se,
    te_pred_up = tau_hat + 1.96 * tau_hat_se
  )] %>%
  .[, .(value, tau_hat, te_pred_down, te_pred_up)] %>%
  .[, method := "CF"]

# cf_vis <- ggplot() +
#     geom_ribbon(data= cf_cate_effect, aes(
#       ymin = te_pred_down,
#       ymax = te_pred_up,
#       x = value), fill = "grey70") +
#   geom_line(data= cf_cate_effect, aes(value, tau_hat)) +
#   labs(
#     x = target_var,
#     y = "Treatment Effects",
#     title = "CF of grf"
#   )
```


# Accelerated Bayesian Causal Forests (XBCF)

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from xbcausalforest import XBCF

NUM_TREES_PR  = 200
NUM_TREES_TRT = 100

xbcf = XBCF(
    #model="Normal",
    parallel=True, 
    num_sweeps=50, 
    burnin=15,
    max_depth=250,
    num_trees_pr=NUM_TREES_PR,
    num_trees_trt=NUM_TREES_TRT,
    num_cutpoints=100,
    Nmin=1,
    #mtry_pr=X1.shape[1], # default 0 seems to be 'all'
    #mtry_trt=X.shape[1], 
    tau_pr = 0.6 * np.var(r.Y)/NUM_TREES_PR, #0.6 * np.var(y) / /NUM_TREES_PR,
    tau_trt = 0.1 * np.var(r.Y)/NUM_TREES_TRT, #0.1 * np.var(y) / /NUM_TREES_TRT,
    alpha_pr= 0.95, # shrinkage (splitting probability)
    beta_pr= 2, # shrinkage (tree depth)
    alpha_trt= 0.95, # shrinkage for treatment part
    beta_trt= 2,
    p_categorical_pr = 0,
    p_categorical_trt = 0,
    standardize_target=True, # standardize y and unstandardize for prediction
         )

T= r.T.astype('int32')

#--- Build an XBCF from a training set ---#
xbcf.fit(
    x_t=r.X_train, # Covariates treatment effect
    x=r.X_train, # Covariates outcome (including propensity score)
    y=r.Y,  # Outcome
    z=T, # Treatment group
)

#--- Calculate the heterogeneous treatment effect ---#
xbcf_te_pred = xbcf.predict(r.X_eval, return_mean=True) 

#--- Calculate 95% credible intervals ---#
tau_posterior = xbcf.predict(r.X_eval, return_mean=False)[:,xbcf.getParams()['burnin']:]

interval_dt = pd.DataFrame(np.quantile(tau_posterior, [0.05,0.95], axis=1).T, columns=['CI_lower', 'CI_upper'])
```


```{r}
xbcf_cate_effect <- tibble("value" = X_eval[, 1], "tau_hat" = py$xbcf_te_pred) %>%
  mutate(
    te_pred_down = py$interval_dt[, "CI_lower"],
    te_pred_up   = py$interval_dt[, "CI_upper"]
  ) %>%
  data.table() %>%
  .[, method := "XBCF"]

# xbcf_vis <- dml_orf_cate_effect %>%
#   ggplot(aes(value, tau_hat)) +
#   geom_ribbon(aes(ymin = te_pred_down, ymax = te_pred_up),
#               fill = "grey70") +
#   geom_line() +
#   labs(
#     x = target_var,
#     y = "Treatment Effects",
#     title = "XBCF"
#   )
```


# Visualization

```{r, fig.width=12, fig.height=6}
report <- bind_rows(dr_orf_cate_effect, dml_orf_cate_effect, cf_cate_effect, xbcf_cate_effect)

ggplot(report) +
  geom_ribbon(aes(
    ymin = te_pred_down,
    ymax = te_pred_up,
    x = value
  ), fill = "grey70") +
  geom_line(aes(value, tau_hat)) +
  facet_grid(~method) +
  labs(
    x = target_var,
    y = "Treatment Effects"
  )
```

## Finding
+ Treatment effects of precipitation estimated by ORF is almost consistent with CF of grf. The estimates from ORF have more tight 95% CI. 
+ The shape of the graph of XBCF is quite angular
  * need to increase the number of trees?
    - Need to read the paper more
















